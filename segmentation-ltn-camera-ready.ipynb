{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install monai > /dev/null\n!pip install itk > /dev/null\n!pip install ltntorch > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def nested(hard):\n    mask1 = hard == 1\n    mask2 = hard == 2\n    mask1 = mask1.unsqueeze(1)\n    mask2 = mask2.unsqueeze(1)\n\n    return sample_and_check(mask1, mask2)\n\ndef sample_and_check(mask1, mask2, num_samples=20):\n    \"\"\"\n    Args:\n    - mask1: A tensor of shape (b, c, h, w, d) where each batch contains a mask.\n    - mask2: A tensor of shape (b, c, h, w, d) where each batch contains a mask.\n    - num_samples: Number of points to sample between each (src, dst) pair.\n    \"\"\"\n    b, c, h, w, d = mask1.shape\n\n    # Initialize the output\n    counts = torch.zeros(b, dtype=torch.int).to(device)\n\n    # Loop through each batch\n    for i in range(b):\n        # Find the indices where mask1 == 1 for the current batch\n        mask1_indices = torch.nonzero(mask1[i] == 1)\n\n        # Randomly sample num_samples (src, dst) pairs\n        if mask1_indices.shape[0] < 2:\n            continue  # Skip if there are fewer than 2 points in mask1 == 1\n\n        src_dst_pairs = mask1_indices[torch.randint(0, mask1_indices.shape[0], (num_samples, 2))]\n\n        for pair_idx, (src, dst) in enumerate(src_dst_pairs):\n            # Get the linearly interpolated points from src to dst\n            src_coords = src[1:]\n            dst_coords = dst[1:]\n\n            # Interpolate between src and dst in each spatial dimension (h, w, d)\n            steps = torch.linspace(0, 1, 50).to(device)\n            interpolated_points = torch.stack([steps * (dst_coords[i] - src_coords[i]) + src_coords[i]\n                                               for i in range(3)], dim=-1)\n\n            # Check for mask2 == 1 at each interpolated point (round coordinates to integers)\n            for point in interpolated_points:\n                # Convert to integers and clamp to valid indices\n                point_rounded = torch.round(point).long()\n                point_rounded = torch.clamp(point_rounded, min=torch.tensor(0).to(device),\n                                            max=torch.tensor([h - 1, w - 1, d - 1]).to(device))\n\n                # Check if mask2 == 1 at this point\n                if mask2[i, 0, point_rounded[0], point_rounded[1], point_rounded[2]] == 1:\n                    counts[i] += 1\n                    break\n            if counts[i] > 0:\n                break\n\n    return counts\n\n\nimport torch\nfrom monai.metrics import DiceMetric\nfrom monai.apps import DecathlonDataset\nfrom monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Spacingd, Resized\nfrom monai.data import partition_dataset\nfrom torch.utils.data import DataLoader, random_split\nfrom monai.networks.nets import SwinUNETR\nfrom monai.losses import DiceLoss\nfrom sklearn.model_selection import KFold\nimport numpy as np\nfrom tqdm import tqdm\nimport ltn\n\ndice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=True)\n\n\ndef evaluate(model, val_loader, device='cuda'):\n    model.eval()\n    dice_metric.reset()\n    with torch.no_grad():\n        for batch in val_loader:\n            images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device).squeeze(1)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1).squeeze(1)\n            dice_metric(preds, labels)\n    dice_score, _ = dice_metric.aggregate()\n    dice_metric.reset()\n    return dice_score.cpu().item()\n\n\n# Data transformation\ntransforms = Compose([\n    LoadImaged(keys=[\"image\", \"label\"]),\n    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n    Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 1.5), mode=(\"bilinear\", \"nearest\")),\n    Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 64), mode=(\"bilinear\", \"nearest\"))\n])\n\n\ndef dimension(pred, gamma=0.0001, epsilon=5000.):\n    hard = pred.argmax(dim=1)\n    mask1 = hard == 1\n    mask2 = hard == 2\n\n    B = mask1.shape[0]  # Batch size\n\n    dims = torch.zeros(B, device=mask1.device)\n\n    for i in range(B):\n        n1 = torch.sum(mask1[i] > 0).item()\n        n2 = torch.sum(mask2[i] > 0).item()\n        diff = torch.clamp((torch.abs(torch.tensor(n1 - n2)) - epsilon), min=0)\n\n        dims[i] = torch.exp(torch.tensor(-gamma * (diff) ** 2))\n\n    return dims\n\n\ndef chamfer_distance(pred):\n    \"\"\"\n    Computes a differentiable Chamfer distance between two binary masks.\n\n    Args:\n    - mask1, mask2: (h, w, d) binary PyTorch tensors\n\n    Returns:\n    - chamfer_dist: scalar differentiable Chamfer distance\n    \"\"\"\n\n    hard = pred.argmax(dim=1)\n    mask1 = hard == 1\n    mask2 = hard == 2\n\n    B = mask1.shape[0]  # Batch size\n    chamfer_dists = torch.zeros(B, device=mask1.device)\n\n    for i in range(B):\n        # Get nonzero coordinates (foreground points) for each sample\n        coords1 = torch.nonzero(mask1[i], as_tuple=False).float()  # (N1, 3)\n        coords2 = torch.nonzero(mask2[i], as_tuple=False).float()  # (N2, 3)\n\n        if coords1.numel() == 0 or coords2.numel() == 0:  # Handle empty masks\n            chamfer_dists[i] = float('inf')\n            continue\n\n        # Compute pairwise Euclidean distances\n        dists = torch.cdist(coords1, coords2, p=2)  # (N1, N2)\n\n        # Get min distances for both directions\n        min_dists1 = torch.min(dists, dim=1)[0]  # (N1,)\n        min_dists2 = torch.min(dists, dim=0)[0]  # (N2,)\n\n        # Chamfer distance for the batch element\n        chamfer_dists[i] = torch.mean(min_dists1) + torch.mean(min_dists2)\n\n    return chamfer_dists  # Shape (B,)\n\n\ndataset = DecathlonDataset(root_dir=\"./\", task=\"Task04_Hippocampus\", section=\"training\", download=True,\n                           transform=transforms)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfrom monai.losses import DiceLoss\n\n# Loss function (Dice Loss is commonly used for segmentation)\nbaseline_loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n\n\ndef baseline_train(model, train_loader, val_loader, num_epochs=20, loss_fn=None):\n    from monai.optimizers import WarmupCosineSchedule\n    from monai.data import DataLoader\n    from torch.optim import AdamW\n\n    # Optimizer\n    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n\n    # steps = num_epochs * len(train_loader)\n    # Learning rate scheduler\n    # scheduler = WarmupCosineSchedule(optimizer, warmup_steps=steps // 10, t_total=steps)\n    scheduler = WarmupCosineSchedule(optimizer, warmup_steps=10, t_total=100)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        for batch in train_loader:\n            inputs, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        print(f\"BASE Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n        dice_score = evaluate(model, val_loader, device)\n        print(f'val dice score: {dice_score}')\n        # train_dice_score = evaluate(model, train_loader, device)\n        # print(f'train dice score: {train_dice_score}')\n        scheduler.step()\n    return model\n\n\nimport ltn\n\ndef ltn_train(model, train_loader, val_loader, num_epochs=20):\n    from monai.optimizers import WarmupCosineSchedule\n    from monai.data import DataLoader\n    from torch.optim import AdamW\n\n    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n    scheduler = WarmupCosineSchedule(optimizer, warmup_steps=10, t_total=100)\n\n    segmentator = ltn.Function(model)\n\n    Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier='f')\n    SatAgg = ltn.fuzzy_ops.SatAgg()\n\n    l_background = ltn.Constant(torch.tensor(0.))\n    l_anterior = ltn.Constant(torch.tensor(1.))\n    l_posterior = ltn.Constant(torch.tensor(2.))\n\n    def eq_fn3d(u, v, alpha=0.3):\n        return torch.exp(-alpha * torch.sqrt(torch.sum(torch.square(u - v), dim=1))).mean(dim=(1, 2, 3))\n\n    def eq_fn(u, v, alpha=1e-3):\n        return torch.exp(-alpha * torch.sqrt(torch.square(u - v)))\n\n    Eq3d = ltn.Predicate(func=eq_fn3d)\n    Eq = ltn.Predicate(func=eq_fn)\n\n    dice_loss = DiceLoss(to_onehot_y=True, softmax=True, reduction='none')\n\n    def my_dice_loss(outputs, labels):\n        return 1. - dice_loss(outputs, labels).mean(dim=1)\n\n    Dl = ltn.Predicate(func=my_dice_loss)\n\n    MinDst = ltn.Function(func=chamfer_distance)\n    SimDim = ltn.Function(func=dimension)\n    Nested = ltn.Function(func=nested)\n    Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        for batch in train_loader:\n            inputs, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n            optimizer.zero_grad()\n\n            # ltn!\n            # we ground the variables with current batch data\n            x = ltn.Variable(\"x\", inputs)\n            y = ltn.Variable(\"y\", labels)\n            ll = ltn.Constant(labels)\n            y_background = ltn.Variable(\"y_background\", labels[labels == 0])\n            y_anterior = ltn.Variable(\"y_anterior\", labels[labels == 1])\n            y_posterior = ltn.Variable(\"y_posterior\", labels[labels == 2])\n\n            outputs = model(inputs)\n            pred = torch.argmax(outputs, dim=1)\n            pred = ltn.Variable(\"pred\", pred)\n\n            zero = ltn.Constant(torch.tensor(0.))\n\n            # ltn!\n            sat_agg = SatAgg(\n                Forall(ltn.diag(x, y), Dl(segmentator(x), y)).value,\n                Forall(pred, Eq(MinDst(pred), zero)).value,\n                Forall(pred, SimDim(pred)).value,\n                Forall(pred, Not(Nested(pred))).value\n            )\n            loss = 1. - sat_agg\n\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        print(f\"LTN Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n        dice_score = evaluate(model, val_loader, device)\n        print(f'val dice score: {dice_score}')\n        # train_dice_score = evaluate(model, train_loader, device)\n        # print(f'train dice score: {train_dice_score}')\n        scheduler.step()\n    return model\n\n\n# k-fold CV execution\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(device)\nbaseline_dice_scores = []\nltn_dice_scores = []\n\nnum_epochs = 100\n\ntr_size = 0.25\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n    print(f\"Fold {fold + 1}/k\")\n\n    train_idx = train_idx[:int(tr_size * len(train_idx))]\n\n    train_subset = torch.utils.data.Subset(dataset, train_idx)\n    val_subset = torch.utils.data.Subset(dataset, val_idx)\n    train_loader = DataLoader(train_subset, batch_size=4, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=4, shuffle=False)\n\n    # LTN Model\n    model_ltn = SwinUNETR(img_size=(64, 64, 64), in_channels=1, out_channels=3, use_checkpoint=True).to(device)\n    trained_model_ltn = ltn_train(model_ltn, train_loader, val_loader, num_epochs=num_epochs)\n    torch.save(trained_model_ltn.state_dict(), f'model_state_dict_ltn_fold{fold + 1}-tr={tr_size}.pth')\n    ltn_dice_scores.append(evaluate(trained_model_ltn, val_loader, device))\n\n    # Baseline Model\n    model = SwinUNETR(img_size=(64, 64, 64), in_channels=1, out_channels=3, use_checkpoint=True).to(device)\n    trained_model = baseline_train(model, train_loader, val_loader, num_epochs=num_epochs, loss_fn=baseline_loss_function)\n    torch.save(trained_model.state_dict(), f'model_state_dict_baseline_fold{fold+1}-tr={tr_size}.pth')\n    baseline_dice_scores.append(evaluate(trained_model, val_loader, device))\n    \n\n# Reporting final results\nbaseline_mean = np.mean(baseline_dice_scores)\nbaseline_std = np.std(baseline_dice_scores)\nltn_mean = np.mean(ltn_dice_scores)\nltn_std = np.std(ltn_dice_scores)\n\nprint(f\"Baseline Model - Mean Dice: {baseline_mean:.4f} ± {baseline_std:.4f}\")\nprint(f\"LTN Model - Mean Dice: {ltn_mean:.4f} ± {ltn_std:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}